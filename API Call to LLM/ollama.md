Ollama gives me an environment that gives me a full wrapper around an LLM that I can either chat with in the terminal or use as a server that I can HTTP POST to and read the output from. 


